---
title: "Final_Project"
author: "Team 5"
date: "2024-11-21"
output:
  
  
 html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_float: yes
---

```{r include=FALSE}
library(ezids)
library(dplyr)
library(tidyverse)
library(ggplot2)
#install.packages("naniar")
library(naniar)
library(gridExtra)
library(car)
```



# EDA

## Data Cleaning

### Understand the Dataset
```{r}
# Loading the dataset
top_songs <- read.csv('universal_top_spotify_songs.csv', na.strings = c("", "NA"))

# Print out the first 5 rows of the dataset
xkabledplyhead(top_songs,title="The first 5 rows of the dataset")
```


```{r include=FALSE}
str(top_songs)
```


```{r}
colnames(top_songs)
```

```{r}
summary(top_songs)
```

1. Drop columns which are not useful for modelling and have no meaning on the popularity of a song

2. The minimum value of time signature of a song is 0, which is not possible (Remove rows with time signature = 0).

3. According to the data description, NAs in `country` means Global Top 50. Therefore, we will fill NAs with 'Global'.

4. Since the percentage of observations with NAs is relatively low at 0.046% of the observations; therefore, we will drop all the other observations with NAs.


### Drop irrelevant columns

```{r}
remove_col <- c(1) # Add on columns
top_songs <- top_songs[, -remove_col]
```


### Handling Missing Values

Visualize the missing values in each column
```{r}
gg_miss_var(top_songs)
```

Calculate the counts and percentage of missing values in each columns
```{r}
# Counts the missing values in each column
colSums(is.na(top_songs))

# Calculate the percentage of missing values in each column
col_na_percentages <-round(colSums(is.na(top_songs))/dim(top_songs)[1]*100, 3)
col_na_percentages
```


```{r}
# Impute the null values in country with 'Global'
top_songs$country = ifelse(is.na(top_songs$country), 'Global', top_songs$country)

# Drop observations with missing values
df_cleaned <- na.omit(top_songs)

# Drop observations with 0 time signature
df_cleaned <- df_cleaned[df_cleaned$time_signature!=0, ]
```

```{r}
print("Dimension of Dataset before Cleaning")
print(dim(top_songs))
print("Dimension of Dataset after Cleaning")
dim(df_cleaned)
```

### Convert Variables into Correct Data Types

```{r include=FALSE}
str(df_cleaned)
```

```{r}
# Convert columns into date data type
df_cleaned$snapshot_date <- as.Date(df_cleaned$snapshot_date, format = "%Y-%m-%d")
df_cleaned$album_release_date <- as.Date(df_cleaned$album_release_date, format = "%Y-%m-%d")

# Convert columns into factor data type
df_cleaned$time_signature <-factor(df_cleaned$time_signature, level= c(1, 3, 4, 5))
df_cleaned$is_explicit <- factor(as.logical(df_cleaned$is_explicit))
df_cleaned$mode <- factor(df_cleaned$mode)

str(df_cleaned)
```


```{r}
summary(df_cleaned)
```

### Outliers Detection

```{r}
selected_columns <- c(
  "popularity", "duration_ms", "danceability", 
  "energy", "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "tempo")

boxplots_list <- list()

# Create box plots for each selected column
for (col in selected_columns) {
  boxplot <- df_cleaned %>% 
    ggplot(aes(y = .data[[col]])) +
    geom_boxplot(fill = "lightblue") +
    labs(title = paste("Box Plot for", col), y = col) +
    theme_minimal()
  print(boxplot)
}

```
## BASIC EDA


```{r ,echo=TRUE,results='markup'}
spotify_data <- df_cleaned
str(spotify_data)
```


### Top 10 artists among spotify songs

```{r ,echo=TRUE,results='markup'}

artists_list <- spotify_data %>%
  select(artists) %>%
  mutate(artists = strsplit(as.character(artists), ", ")) %>%
  unnest(artists)


artists_count <- artists_list %>%
  group_by(artists) %>%
  summarize(frequency = n()) %>%
  arrange(desc(frequency))


top_artists <- artists_count %>%
  slice_max(frequency, n = 10)


ggplot(top_artists, aes(x = reorder(artists, -frequency), y = frequency)) +
  geom_bar(stat = "identity", fill = "red") +
  theme_minimal() +
  labs(title = "Top 10 Most Common Artists Among top spotify songs",
       x = "Artist",
       y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



### Songs with most appearances
```{r ,echo=TRUE,results='markup'}
top_songs <- spotify_data %>%
  count(name, sort = TRUE) %>%
  slice_max(n, n = 10) %>%
  arrange(n)

# Create the bar chart
ggplot(top_songs, aes(x = reorder(name, n), y = n)) +
  geom_bar(stat = "identity", fill = "#1DB954") +  # Spotify green color
  geom_text(aes(label = n), vjust = -0.5, size = 3) +  # Add labels above bars
  labs(
    title = "Songs with Most Appearances",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  theme(
    text = element_text(size = 14),
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )


```

## SMART QUESTION 1: What factors contribute most to a song's popularity over time?

### Objective: Analyze the relationship between song attributes (danceability, energy, tempo, valence, and loudness) and popularity trends over time.



### Density plot of popularity
```{r ,echo=TRUE,results='markup'}
ggplot(spotify_data, aes(x = popularity)) +
  geom_density(fill = "purple", alpha = 0.5) +
  labs(title = "Density Plot of Popularity", x = "Popularity", y = "Density")
```

### Distribution of variables- popularity, danceability, energy, loudness, tempo, valence

```{r ,echo=TRUE,results='markup'}
library(ggplot2)

# Distribution of popularity
ggplot(spotify_data, aes(x = popularity)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
   geom_density(color = "red", size = 1.2)+
  labs(title = "Distribution of Popularity", x = "popularity", y = "Frequency")

qqnorm(spotify_data$popularity)
qqline(spotify_data$popularity, col = "red", lwd = 2)

# Distribution of danceability
ggplot(spotify_data, aes(x = danceability)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  geom_density(color = "red", size = 1.2)+
  labs(title = "Distribution of Danceability", x = "danceability", y = "Frequency")

qqnorm(spotify_data$danceability)
qqline(spotify_data$danceability, col = "red", lwd = 2)

# Distribution of energy
ggplot(spotify_data, aes(x = energy)) +
  geom_histogram(bins = 30, fill = "red", color = "black") +
  geom_density(color = "green", size = 1.2)+
  labs(title = "Distribution of energy", x = "energy", y = "Frequency")

qqnorm(spotify_data$energy)
qqline(spotify_data$energy, col = "red", lwd = 2)


# Distribution of tempo
ggplot(spotify_data, aes(x = tempo)) +
  geom_histogram(bins = 30, fill = "green", color = "black") +
  geom_density(color = "red", size = 1.2)+
  labs(title = "Distribution of tempo", x = "tempo", y = "Frequency")

qqnorm(spotify_data$tempo)
qqline(spotify_data$tempo, col = "red", lwd = 2)

# Distribution of valence

ggplot(spotify_data, aes(x = valence)) +
  geom_histogram(bins = 30, fill = "yellow", color = "black") +
  geom_density(color = "red", size = 1.2)+
  labs(title = "Distribution of valence", x = "valence", y = "Frequency")

qqnorm(spotify_data$valence)
qqline(spotify_data$valence, col = "red", lwd = 2)

# Distribution of loudness

ggplot(spotify_data, aes(x = loudness)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  geom_density(color = "red", size = 1.2)+
  labs(title = "Distribution of loudness", x = "loudness", y = "Frequency")

qqnorm(spotify_data$loudness)
qqline(spotify_data$loudness, col = "red", lwd = 2)


```



## Not following normal distribution
## Hence let us use 
## Spearman coef technique

```{r ,echo=TRUE,results='markup'}
spotify_num <- spotify_data %>%
  select(popularity, danceability, energy, tempo, valence, loudness)

# Calculate Spearman's rank correlation
cor_matrix_spearman <- cor(spotify_num, use = "complete.obs", method = "spearman")
print("Spearman Correlation Matrix:")
print(cor_matrix_spearman)

# Visualize Spearman correlation
corrplot::corrplot(cor_matrix_spearman, method = "color", addCoef.col = "black", title = "Spearman Correlation", mar = c(0, 0, 1, 0))

```
# If we consider factors like loudness, valence, Popularity has weak correlations with all other factors when considered alone.
# There are factors like loudness and energy which has strong colineratity with each other. there are some attributes with weak correlation.
# Combining these affects might impact the popularity.



### Regression Problem

## Interactive models

```{r ,echo=TRUE,results='markup'}
interaction_model1 <- lm(popularity ~ loudness * danceability + liveness * acousticness + speechiness *energy , data = spotify_data)
summary(interaction_model1)
vif(interaction_model1, type = "predictor")
```

```{r ,echo=TRUE,results='markup'}
interaction_model2 <- lm(popularity ~ loudness * danceability + energy, data = spotify_data)
summary(interaction_model2)
vif(interaction_model2, type = "predictor")
```

```{r ,echo=TRUE,results='markup'}
interaction_model3 <- lm(popularity ~ loudness + danceability * energy, data = spotify_data)
summary(interaction_model3)
vif(interaction_model3, type = "predictor")
```

### Comparing different models

```{r ,echo=TRUE,results='markup'}
anova_results2 <- anova(interaction_model1, interaction_model2, interaction_model3)
print(anova_results2)
```


## Proceeding with interactive_model1
### Checking for non-linearity in our best model so far

```{r ,echo=TRUE,results='markup'}
residuals <- residuals(interaction_model1)
```

```{r ,echo=TRUE,results='markup'}
ggplot(data.frame(residuals), aes(x = residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "blue", color = "black") +
  geom_density(color = "red", size = 1.2) +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")
```

```{r ,echo=TRUE,results='markup'}
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
```

```{r ,echo=TRUE,results='markup'}
plot(fitted(interaction_model1), residuals,
     main = "Residuals vs Fitted Values",
     xlab = "Fitted Values",
     ylab = "Residuals")
abline(h = 0, col = "red", lwd = 2)
```

# The histogram of residuals shows skewness, indicating potential non-normality.
# The Q-Q plot reveals deviations from the normality assumption at both ends.
# There's evidence of heteroscedasticity (non-constant variance of residuals), as the spread of residuals increases with fitted values.
## None of the variables appear strictly normally distributed based on the

## Need to address the above potential concerns

### Cooks distance
```{r ,echo=TRUE,results='markup'}
cooks_dist <- cooks.distance(interaction_model1)
influential_points <- which(cooks_dist > (4 / nrow(spotify_data)))
spotify_data_clean <- spotify_data[-influential_points, ]

```


### Square root transformation
```{r ,echo=TRUE,results='markup'}
spotify_data_clean$loudness_scaled <- scale(spotify_data_clean$loudness)
spotify_data_clean$sqrt_energy <- sqrt(spotify_data_clean$energy)
spotify_data_clean$log_popularity <- log(spotify_data_clean$popularity + 1)
spotify_data_clean$log_liveness <- log(spotify_data_clean$liveness + 1)
spotify_data_clean$log_speechiness <- log(spotify_data_clean$speechiness + 1)

transformed_model2 <- lm(log_popularity ~ loudness_scaled*danceability + energy*valence + log_liveness * acousticness + log_speechiness, data = spotify_data_clean)

summary(transformed_model2)
vif(transformed_model2, type = 'predictor')


```



### plotting residuals vs fitted for new model
```{r ,echo=TRUE,results='markup'}
plot(fitted(transformed_model2), residuals(transformed_model2),
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals")
abline(h = 0, col = "red")

```


```{r ,echo=TRUE,results='markup'}
residuals <- residuals(transformed_model2)
```

```{r ,echo=TRUE,results='markup'}
ggplot(data.frame(residuals), aes(x = residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "blue", color = "black") +
  geom_density(color = "red", size = 1.2) +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")
```

```{r ,echo=TRUE,results='markup'}
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
```


```{r ,echo=TRUE,results='markup'}
library(mgcv)
library(caret)
library(dplyr)
library(MASS)  # For polr
library(brant) # For Brant test
library(ggplot2)
library(smotefamily)
```



```{r ,echo=TRUE,results='markup'}
# Convert popularity to an ordinal factor
spotify_data_clean$popularity_factor <- cut(spotify_data_clean$popularity,
                                            breaks = c(-Inf, 33, 66, Inf),
                                            labels = c("Low", "Medium", "High"),
                                            ordered_result = TRUE)
```


```{r ,echo=TRUE,results='markup'}

# Split the data
set.seed(123) 
train_index <- createDataPartition(spotify_data_clean$popularity_factor, p = 0.8, list = FALSE)
train_data <- spotify_data_clean[train_index, ]
test_data <- spotify_data_clean[-train_index, ]

# Check the class distribution in train and test data
table(train_data$popularity_factor)
table(test_data$popularity_factor)
```



```{r ,echo=TRUE,results='markup'}
# Check class distribution
table(train_data$popularity_factor)

# Visualize class distribution
library(ggplot2)
ggplot(data.frame(popularity_factor = train_data$popularity_factor), 
       aes(x = popularity_factor)) +
  geom_bar(fill = "skyblue") +
  labs(title = "Class Distribution of Popularity", x = "Popularity Category", y = "Count")

```

```{r ,echo=TRUE,results='markup'}
# use smote as classes are imbalanced- very negligivble values for low category
# Balance the classes using SMOTE

# Split data by class
low_class <- subset(train_data, popularity_factor == "Low")
medium_class <- subset(train_data, popularity_factor == "Medium")
high_class <- subset(train_data, popularity_factor == "High")

# Find the maximum class size
max_samples <- max(nrow(medium_class), nrow(high_class))

# Randomly oversample minority classes to match the majority class size
set.seed(123)  # For reproducibility
low_class <- low_class[sample(nrow(low_class), max_samples, replace = TRUE), ]
medium_class <- medium_class[sample(nrow(medium_class), max_samples, replace = TRUE), ]

# Combine all classes to create the balanced dataset
oversampled_train_data <- rbind(low_class, medium_class, high_class)

table(oversampled_train_data$popularity_factor)

ggplot(data.frame(popularity_factor = oversampled_train_data$popularity_factor), 
       aes(x = popularity_factor)) +
  geom_bar(fill = "skyblue") +
  labs(title = "Class Distribution After Oversampling", 
       x = "Popularity Category", 
       y = "Count")


```

# 2. Ordinal logistic regression model

```{r ,echo=TRUE,results='markup'}
# Install and load the MASS package
library(MASS)

oversampled_train_data$loudness_scaled <- scale(oversampled_train_data$loudness)
oversampled_train_data$sqrt_energy <- sqrt(oversampled_train_data$energy)
oversampled_train_data$log_popularity <- log(oversampled_train_data$popularity + 1)
oversampled_train_data$log_liveness <- log(oversampled_train_data$liveness + 1)
oversampled_train_data$log_speechiness <- log(oversampled_train_data$speechiness + 1)


ordinal_model <- polr(popularity_factor ~ loudness_scaled*danceability + energy*valence + log_liveness + log_speechiness, data = oversampled_train_data, Hess = TRUE)

# Model summary
summary(ordinal_model)

# Get odds ratios
exp(coef(ordinal_model))

```

```{r ,echo=TRUE,results='markup'}
# Install and load the brant package
library(brant)
# Perform the Brant test
brant(ordinal_model)

```

```{r ,echo=TRUE,results='markup'}
# Make predictions on test data
predicted_classes <- predict(ordinal_model, newdata = test_data, type = "class")

# Confusion Matrix
confusionMatrix <- table(Predicted = predicted_classes, Actual = test_data$popularity_factor)
confusionMatrix

# Accuracy
accuracy <- sum(diag(confusionMatrix)) / sum(confusionMatrix)
cat("Accuracy:", accuracy, "\n")

# Precision, Recall, and F1-Score (for each class)
precision <- diag(confusionMatrix) / colSums(confusionMatrix)
recall <- diag(confusionMatrix) / rowSums(confusionMatrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")
```

```{r ,echo=TRUE,results='markup'}

selected_features <- c("danceability", "loudness", "energy", 
                       "valence", "acousticness", "speechiness", "liveness")

X_train <- oversampled_train_data[, selected_features]
y_train <- oversampled_train_data$popularity_factor


X_test <- test_data[, selected_features]
y_test <- test_data$popularity_factor
```



## 3. RANDOM FOREST MODEL (Final model)

```{r ,echo=TRUE,results='markup'}
memory.limit(size = 30000)
library(randomForest)

set.seed(123)  # For reproducibility
subset_indices <- sample(1:nrow(X_train), size = 500000)  # Adjust size as needed
X_train_subset <- X_train[subset_indices, ]
y_train_subset <- y_train[subset_indices]

# Train the Random Forest on the subset
rf_model <- randomForest(
  x = X_train_subset,
  y = y_train_subset,
  ntree = 500,
  mtry = sqrt(ncol(X_train_subset)),
  importance = TRUE
)

print(rf_model)

```

### FEATURE IMPORTANCE
```{r ,echo=TRUE,results='markup'}
# Visualize feature importance
varImpPlot(rf_model, main = "Feature Importance in Random Forest")
```
## PREDICTION

```{r ,echo=TRUE,results='markup'}
# Make predictions on test data
y_pred <- predict(rf_model, X_test)
```

### CONFUSION MATRIX

```{r ,echo=TRUE,results='markup'}
# Evaluate the model
library(caret)
conf_matrix <- confusionMatrix(factor(y_pred), factor(y_test))
print(conf_matrix)

```


### EVALUATION METRICES

```{r ,echo=TRUE,results='markup'}
# Extract the class-level metrics
precision_per_class <- conf_matrix$byClass[, "Pos Pred Value"]
recall_per_class <- conf_matrix$byClass[, "Sensitivity"]

# Handle potential NAs in metrics (in case of missing classes in predictions)
precision_per_class[is.na(precision_per_class)] <- 0
recall_per_class[is.na(recall_per_class)] <- 0

# Compute F1-Score for each class
f1_per_class <- 2 * ((precision_per_class * recall_per_class) / 
                     (precision_per_class + recall_per_class))
f1_per_class[is.na(f1_per_class)] <- 0  # Handle NaNs in F1-Score

# Compute class proportions in the test set
class_counts <- table(y_test)
total_counts <- sum(class_counts)
class_weights <- class_counts / total_counts

# Compute weighted averages
weighted_precision <- sum(precision_per_class * class_weights)
weighted_recall <- sum(recall_per_class * class_weights)
weighted_f1 <- sum(f1_per_class * class_weights)

# Print metrics
cat("Accuracy:", conf_matrix$overall['Accuracy'], "\n")
cat("Weighted Precision:", weighted_precision, "\n")
cat("Weighted Recall:", weighted_recall, "\n")
cat("Weighted F1-Score:", weighted_f1, "\n")
```

# SMART QUESTION 2
## Can musical features determine a song ranked higher (top 25) or lower (25-50) on the global rank?

```{r}

# Prepare the Dataset
# Filter out only the global top 50 songs
global <- df_cleaned %>% filter(df_cleaned$country == "Global")

global <- global %>% mutate(
  top_25 =ifelse(global$daily_rank < 26, TRUE, FALSE),
  age = 
    as.numeric(format(global$snapshot_date, "%Y")) - as.numeric(format(global$album_release_date, "%Y")),
   duration_min = duration_ms/1000/60                                                
  )

str(global)

```


# EDA

## All Numerical Features
```{r}
library(plotly)

colnames(global)

selected_columns <- c( "daily_movement", "weekly_movement",
  "duration_min", "danceability", 
  "energy", "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "tempo", "popularity", "age")

for (col in selected_columns) {
  
  # histogram
  hist <- global %>% ggplot(aes(x=.data[[col]], color=top_25)) +
  geom_histogram(fill = "white", alpha = 0.5)+
  scale_color_brewer(palette="Dark2")
  print(hist)

  # Boxplot
  boxplot <- global %>% ggplot(aes(x = top_25, y=.data[[col]], fill=top_25)) +
  geom_boxplot()
  print(ggplotly(boxplot))
}

top25 <- global %>% filter(top_25 == TRUE)
top50 <- global %>% filter(top_25 == FALSE)

for (col in selected_columns) {
  # t-test
  print(paste("T-test result for", col, ":"))
  print(t.test(top25[[col]], top50[[col]]))
}

```

## Musical Features

```{r}
num_columns <- c(
  "daily_rank", "danceability", 
  "energy", "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "tempo")

# Select relevant columns for the pair plot
df_selected <- global[, num_columns]



loadPkg("psych") # pair plots with histogram on diagonal and other options
pairs.panels(df_selected, 
             method = "spearman", # correlation method
             hist.col = "#00AFBB",
             density = FALSE,  # show density plots
             ellipses = FALSE, # show correlation ellipses,
             main = "Pair Plot of Musical Features"
             )
unloadPkg("psych")

```



```{r fig.width=30, fig.height=20}

colnames(global)

selected_columns <- c("danceability", 
  "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "energy", "tempo")

boxplot_list <- list()

for (col in selected_columns) {
  
  # Boxplot
  boxplot <- global %>% ggplot(aes(x = top_25, y = .data[[col]], fill = top_25)) +
    geom_boxplot() +
    theme(legend.position = "none")+
    labs(title = paste("Boxplot of", col))
  
  boxplot_list[[col]] <- boxplot
}

# Combine boxplots into grids
boxplot_grid <- subplot(boxplot_list, nrows = 3, margin = 0.05, titleX = TRUE, titleY = TRUE)

# Add a title to the grid
boxplot_grid <- boxplot_grid %>%
  layout(
    title = list(
      text = "Boxplot of Rank Groups vs Musical Features",
      font = list(size = 16, color = "black", family = "Arial"),
      x = 0.5,  # Center the title
      xanchor = "center"
    )
  )


boxplot_grid

```


According to t-test, the p-value of `tempo` and `energy` are not less than 0.05. There is no sufficient evidence to conclude that there is a significant difference in `tempo` and `energy` between the Top 25 and Top 26-50 groups. With all the p-values of other musical features (`loudness`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`,`valence`, `danceability`) less than 0.05, we can reject the null and conclude that there is a significant difference in these musical features (`loudness`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`,`valence`, `danceability`) between the Top 25 and Top 26-50 groups

Besides musical features, we also explore on other performance metrics, `daily_movement`, `weekly_movement` and `popularity`. __Daily and Weekly Movements__: Songs in the Top 25 show higher variability and more positive movement outliers, suggesting these songs may have larger shifts in rankings. __Popularity__: There is little difference in the popularity distributions between the Top 25 and Top 26-50 groups, implying that popularity might not be a strong differentiating factor for Top 25 songs. However, according to t-test, we can conclude that there is sufficient evidence to conclude that there is a significant difference in these musical featuers between the Top 25 and Top 26-50 groups.

Lastly, we will look at the `age` of song, which is defined as the year difference between the year the album released and the year it is ranked on the top 50 songs. We cannot observe much in the boxplot. The IQR for both groups is narrow and close to 0, suggesting that most songs are new.


# Modeling

## Logistic Regression

```{r}
loadPkg("leaps")

global$top_25 <- factor(global$top_25)

colnames(global)

global_cleaned <- global[, -c(1,2,3,6,7,10,11,12)]

str(global_cleaned)

# Put target variable `top_25` to the end of columns
global_cleaned <- global_cleaned[, c(setdiff(names(global_cleaned), "top_25"), "top_25")]

str(global_cleaned)

top25Logit0 = glm(top_25 ~1, data = global_cleaned, family = "binomial")
# Build model using all x varibles
top25Logit1 = glm(top_25 ~., data = global_cleaned, family = "binomial")

xkabledply(top25Logit1)
```

```{r include=FALSE}
# Backward Selection
step(top25Logit1)
# only excluding tempo
```


**Model (Backward Feature Selection)**
Call:  glm(formula = top_25 ~ daily_movement + weekly_movement + popularity + 
    is_explicit + duration_min + danceability + energy + key + 
    loudness + mode + speechiness + acousticness + instrumentalness + 
    liveness + valence + time_signature + age, family = "binomial", 
    data = global_cleaned)


## Forward Selection
```{r include=FALSE}

step(top25Logit0, scope = list(upper = top25Logit1, lower = top25Logit0), 
    method = "forward")

# Only excluding tempo
```

**Model (Forward Feature Selection)**

Call:  glm(formula = top_25 ~ weekly_movement + popularity + daily_movement + 
    danceability + key + speechiness + duration_ms + instrumentalness + 
    time_signature + acousticness + loudness + age + mode + liveness + 
    is_explicit + valence + energy, family = "binomial", data = global_cleaned)

    
    
```{r}

top25Logit_final = glm(
  formula = top_25 ~ weekly_movement + daily_movement + 
    danceability + key + speechiness + duration_min + instrumentalness + 
    acousticness + loudness + age + mode + liveness + 
    is_explicit + valence, family = "binomial", data = global_cleaned)


xkablevif(top25Logit_final)
```

After checking the Variance Inflation Factor (VIF), we identified the presence of multicollinearity in our model. To address this, we will remove variables with the highest VIF in a stepwise manner, starting with time_signature, energy, and popularity, to ensure all VIF values are below 10.


```{r}
# Model after removing variables > 10
top25Logit_final = glm(formula = top_25 ~ weekly_movement + daily_movement + 
    danceability + key + speechiness + duration_min + 
    instrumentalness + acousticness + loudness + age + mode + liveness + 
    is_explicit + valence, family = "binomial", data = global_cleaned)

summary(top25Logit_final)
```


```{r results='markup'}
xkabledply(top25Logit_final)
```

According to the summary of logistic regression, we will then drop the insignificant predictor `valence`.


```{r}

# Final Model
top25Logit_final = glm(
  formula = top_25 ~ weekly_movement + daily_movement + 
    danceability + key + speechiness + duration_min + instrumentalness + 
    acousticness + loudness + age + mode + liveness+
    is_explicit, family = "binomial", data = global_cleaned)

xkabledply(top25Logit_final)
summary(top25Logit_final)
```


```{r fig.width=40, fig.height=8}

# Exponentiate the estimated coefficients of logistic regression
expcoeff = exp(coef(top25Logit_final))
# expcoeff
xkabledply( as.table(expcoeff), title = "Exponential of coefficients in Logit Reg" )
```

__Musical Features__<br>
Notably, we can observe a strong positive relationship between __danceability__ and higher rankings.The odds of being in the Top 25 increase by approximately 750.5%. `key` and `mode1` all have positive impact association with higher rankings. While `speechiness`, `instrumentalness`, `acusticness`, `loudness` and `liveness` has negative association with higher rankings.

__Non Musical Features__<br>
`duration_min` has positive impact association with higher rankings.  Meanwhile, the exponential coefficient of `is_explicitTRUE` is less than 1 meaning that being an explicit song (TRUE) decreases the odds of being top 25 ranking by 11.38%.

__Performance Metric__<br>
Among performance metrics, both `weekly_movement` and `daily_movement` are positively associated with higher rankings, suggesting that better movement trends contribute to achieving higher chart positions.


### Final Model Performance ###
```{r, results='markup'}
loadPkg("caret")
loadPkg("pROC")
loadPkg("pscl")

SongLogit <-glm(
  formula = top_25 ~ weekly_movement + daily_movement + 
    danceability + key + speechiness + duration_min + instrumentalness + 
    acousticness + loudness + age + mode + liveness + 
    is_explicit, family = "binomial", data = global_cleaned)


summary(SongLogit)

# Convert fitted probabilities to predicted classes using a 0.5 threshold
global_cleaned$predicted <- as.factor(ifelse(SongLogit$fitted.values > 0.5, 1, 0))

# Ensure the reference variable (true labels) is a factor
global_cleaned$y <- as.factor(ifelse(global_cleaned$top_25==TRUE,1,0))


conf_matrix <- confusionMatrix(data = global_cleaned$predicted, 
                               reference = global_cleaned$y,
                               positive = "1")
print(conf_matrix)

prob = predict(SongLogit, type = "response")
h <- roc(global_cleaned$top_25~prob)
print(auc(h))
plot(h, main = "Logistic Regression ROC Curve", col = "blue", lwd = 3)
SongNullLogit <- glm(top_25 ~ 1, family = "binomial", data = global_cleaned)
mcFadden = 1 - logLik(SongLogit)/logLik(SongNullLogit)
mcFadden
pR2(SongLogit)

unloadPkg("caret")
unloadPkg("pROC")
unloadPkg("pscl")

colnames(global_cleaned)

# Drop `predicted` and `y` column
global_cleaned <- global_cleaned[, c(-20, -21)]
str(global_cleaned)
```

         
__Model Evaluation on Logistic Regression__

The logistic regression model demonstrates moderate accuracy at 61.4%, with stronger sensitivity (64.6%) compared to weaker specificity (58.1%). This imbalance suggests that the model is biased towards predicting class 0.

The AUC is 0.677, which falls below the threshold of 0.8, indicating a relatively low ability to distinguish between classes. Additionally, the McFadden's RÂ² value is only 0.0832, meaning the model explains just 8.32% of the variation in the target variable. These metrics suggest that the model does not provide a good fit. Therefore, exploring alternative modeling techniques is recommended.


## KNN

```{r}
# Prepare dataframe for KNN
df_knn <- global_cleaned

# Put target variable `top_25` to the end of columns
df_knn <- df_knn[, c(setdiff(names(df_knn), "top_25"), "top_25")]


# convert all X variables into numeric data types
df_knn$is_explicit = as.numeric(df_knn$is_explicit)
df_knn$mode = as.numeric(df_knn$mode)
df_knn$time_signature = as.numeric(df_knn$time_signature)

str(df_knn)
str(df_knn[c(-19)])
```


### Full Model

```{r}

scaledglobal <- as.data.frame(scale(df_knn[c(-19)], center = TRUE, scale = TRUE))
set.seed(321)
global_sample <- sample(2, nrow(scaledglobal), replace=TRUE, prob=c(0.75, 0.25))

train_X <- scaledglobal[global_sample==1, 1:18]
test_X <- scaledglobal[global_sample==2, 1:18]

train_y <- df_knn[global_sample==1, 19]
test_y <- df_knn[global_sample==2, 19]
```

#### Tuning: Test set

```{r}
loadPkg("FNN")
loadPkg("gmodels") # CrossTable
loadPkg("caret") # confusionMatrix
ResultDf = NULL

unique(test_y)

for (kval in 1:11) {
  rank_pred <- knn(train = train_X, test = test_X, cl=train_y, k=kval)
  RankCross <- CrossTable(test_y, rank_pred, prop.chisq = FALSE)
  print( paste("k = ", kval) )
  RankCross
  # 
  cm = confusionMatrix(rank_pred, reference = test_y, positive = "TRUE" ) # from caret library
  # print.confusionMatrix(cm)
  # 
  cmaccu = cm$overall['Accuracy']
  print( paste("Total Accuracy = ", cmaccu ) )
  # print("Other metrics : ")
  # print(cm$byClass)
  # 
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  # cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  ResultDf = rbind(ResultDf, cmt)
  print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
  print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}

unloadPkg("FNN")
unloadPkg("gmodels") 
unloadPkg("caret")

```

```{r, results="asis"}
xkabledply(ResultDf, "Total Accuracy Summary")
```



```{r}
loadPkg("gmodels")
loadPkg("FNN")
# re-do that one with max accuracy
rank_pred_m <- knn(train = train_X, test = test_X, cl=train_y, k=3)
RankCross_m <- CrossTable(test_y, rank_pred_m, prop.chisq = FALSE)
unloadPkg("FNN")
unloadPkg("gmodels")
```

```{r}
loadPkg("caret") 
cmknn = confusionMatrix(rank_pred_m, reference = test_y, positive = "TRUE")
cmknn
print('Overall: ')
cmknn$overall
print('Class: ')
cmknn$byClass
unloadPkg("caret")
```



#### Tuning: Cross Validation

```{r}
loadPkg("caret")

# Tuning: Finding the best k
knnModel <- train(
  x = train_X, 
  y = train_y, 
  method = "knn", 
  trControl = trainControl(method = "cv"), 
  tuneGrid = data.frame(k = c(3, 5, 7, 9, 11, 13))
)


best_model<- knn3(
                  x = train_X,
                  y = train_y,
                  k = knnModel$bestTune$k
                 )

best_model

# Model Evaluation
predictions <- predict(best_model, test_X,type = "class")

# Calculate confusion matrix
cm <- confusionMatrix(predictions, test_y, positive = "TRUE")
cm
unloadPkg("caret")
```


#### Final Model Evaluation
```{r}
loadPkg("pROC")
loadPkg("FNN")


# Generate probabilities for KNN
probabilities <- knn(train = train_X, test = test_X, cl = train_y, k = 3, prob = TRUE)
prob_positive <- attr(probabilities, "prob")
prob_positive <- ifelse(probabilities == "TRUE", prob_positive, 1 - prob_positive)

# Calculate and plot ROC curve
roc_curve <- roc(test_y, prob_positive)
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
plot(roc_curve, main = "KNN ROC Curve (Full Model)", col = "blue", lwd = 2)
```

__Model Evaluation on KNN (Full Model)__<br>

We started by building a model using all features and employed both a train-test split and cross-validation to tune and validate the performance. For the test set, k=2, achieved the highest accuracy (0.850). However, to balance high accuracy with the risk of overfitting, k=3, achieved the highest accuracy (0.850). However, to balance high accuracy with the risk of overfitting, k=3, consistently delivered the best performance across all metrics, including total accuracy, specificity, and sensitivity, each averaging around 85%. The model demonstrates balanced performance across both classes, with sensitivity, specificity, and precision all near 85%. Additionally, the AUC score of 0.913 (above the 0.8 threshold) confirms that the model is a strong fit for the data.


### Musical Characteristics

Now we only select musical characteristics (`danceability`,`energy`, `key`, `loudness`, `mode`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`, `valence`, `tempo`, `time_signature`)

```{r}

colnames(df_knn)

# Select only Musical Characteristics
scaledglobal_music <- as.data.frame(scale(df_knn[,5:16], center = TRUE, scale = TRUE))

scaledglobal_music

set.seed(321)
global_sample_music <- sample(2, nrow(scaledglobal_music), replace=TRUE, prob=c(0.75, 0.25))

train_X <- scaledglobal_music[global_sample_music==1, ]
test_X <- scaledglobal_music[global_sample_music==2, ]

train_y <- df_knn[global_sample_music==1, 19]
test_y <- df_knn[global_sample_music==2, 19]

```

#### Tuning: Test Set

```{r}
loadPkg("FNN")
loadPkg("gmodels") # CrossTable
loadPkg("caret") # confusionMatrix
ResultDf = NULL
for (kval in 1:15) {
  rank_pred <- knn(train = train_X, test = test_X, cl=train_y, k=kval)
  RankCross <- CrossTable(test_y, rank_pred, prop.chisq = FALSE)
  print( paste("k = ", kval) )
  RankCross
  # 
  cm = confusionMatrix(rank_pred, reference = test_y, positive = "TRUE") # from caret library
  # print.confusionMatrix(cm)
  # 
  cmaccu = cm$overall['Accuracy']
  print( paste("Total Accuracy = ", cmaccu ) )
  # print("Other metrics : ")
  # print(cm$byClass)
  # 
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  # cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  ResultDf = rbind(ResultDf, cmt)
  print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
  print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}

```

```{r, results="asis"}
xkabledply(ResultDf, "Total Accuracy Summary")
```


#### Tuning: Cross Validation
```{r}
loadPkg("caret")
knnModel <- train(
  x = train_X, 
  y = train_y, 
  method = "knn", 
  trControl = trainControl(method = "cv"), 
  tuneGrid = data.frame(k = c(3, 5, 7, 9, 11, 13))
)

knnModel

best_model<- knn3(
                  x = train_X,
                  y = train_y,
                  k = knnModel$bestTune$k
                 )

# Model Evaluation
predictions <- predict(best_model, test_X,type = "class")
# Calculate confusion matrix
cm <- confusionMatrix(predictions, test_y, positive = "TRUE")
cm

```


```{r}
loadPkg("FNN")
loadPkg("gmodels")
# re-do that one with max accuracy
rank_pred_m <- knn(train = train_X, test = test_X, cl=train_y, k=7)
RankCross_m <- CrossTable(test_y, rank_pred_m, prop.chisq = FALSE)
unloadPkg("FNN")
unloadPkg("gmodels")
```

```{r}
loadPkg("caret") 
cmknn = confusionMatrix(rank_pred_m, reference = test_y, positive = "TRUE")
cmknn
print('Overall: ')
cmknn$overall
print('Class: ')
cmknn$byClass
unloadPkg("caret")
```

```{r}
library(pROC)
loadPkg("FNN")

# Generate probabilities for KNN
probabilities <- knn(train = train_X, test = test_X, cl = train_y, k = 7, prob = TRUE)
prob_positive <- attr(probabilities, "prob")
prob_positive <- ifelse(probabilities == "TRUE", prob_positive, 1 - prob_positive)

# Calculate and plot ROC curve
roc_curve <- roc(test_y, prob_positive)
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
plot(roc_curve, main = "KNN ROC Curve (Musical Characteristics)", col = "blue", lwd = 2)

```

__Model Evaluation on KNN (Musical Features)__<br>

For the test set, k= 7, yield the highest accuracy (0.747). However, cross-validation identified k=3 as the best model, with an accuracy of 75.3% and balanced sensitivity (72.6%) and specificity (77.9%). While k=3 performed slightly better at identifying class 1 than class 0, it maintained a good overall balance in cross-validation.

When comparing the two approaches, we observed different best values for k. Using k=3 n the test set resulted in a larger discrepancy between specificity and sensitivity, which could indicate reduced robustness. Meanwhile, the cross-validation performance for k=7 demonstrated consistent accuracy (around 0.7472), aligning closely with the test set performance for k=7.

Given this consistency and the stability of metrics, we selected k=7 as the best model. The AUC score of 0.8136 (greater than 0.8) further confirms the model's effectiveness and suitability.


__Key Findings and Insights__

Logistic regression reveals that people prefer energetic and danceable music, while niche qualities like live recordings or high speech content have less universal appeal. Comparing k-NN models with all features versus only musical features shows that while musical features are strong predictors, incorporating non-musical factors (e.g., explicit content, duration, age) and performance metrics (e.g., weekly and daily movement) enhances prediction. Overall, k-NN outperforms logistic regression, likely due to its ability to capture non-linear relationships between musical features and rank groups, as observed in the EDA.


### 3. How do musical preferences (song attributes or artists) vary across countries and regions?

#### 3.1 Data Preparation for Clustering 

```{r}

spotify_df <- df_cleaned

# Summarize attributes by country
country_summary <- spotify_df %>%
  group_by(country) %>%
  summarize(
    avg_acousticness = mean(acousticness, na.rm = TRUE),
    avg_danceability = mean(danceability, na.rm = TRUE),
    avg_valence = mean(valence, na.rm = TRUE),
    avg_energy = mean(energy, na.rm = TRUE),
    avg_loudness = mean(loudness, na.rm = TRUE),
    avg_instrumentalness = mean(instrumentalness, na.rm = TRUE),
    avg_tempo = mean(tempo, na.rm = TRUE),
    avg_liveness = mean(liveness, na.rm = TRUE)
  )

country_data_scaled <- scale(country_summary %>% select(-country))

```

#### K-Means Clustering

```{r}
library(factoextra)

# Elbow method to find optimal number of clusters
fviz_nbclust(country_data_scaled, kmeans, method = "wss") +
  labs(title = "Elbow Method for Optimal Clusters")

# Set optimal number of clusters
set.seed(123)
k_optimal <- 5  # Change this to 4 if you'd like to explore fewer clusters
kmeans_result <- kmeans(country_data_scaled, centers = k_optimal)

# Add cluster labels to the original dataset
country_summary$cluster <- as.factor(kmeans_result$cluster)

```

#### PCA Plot

```{r}
# Perform PCA
pca_result <- prcomp(country_data_scaled, center = TRUE, scale. = TRUE)

# Calculate variance explained
variance_explained <- summary(pca_result)$importance[2, ]
cumulative_variance <- cumsum(variance_explained)

# Identify the number of PCs to explain > 70% variance
pcs_needed <- which(cumulative_variance >= 0.7)[1]

cat("Number of PCs explaining > 70% variance:", pcs_needed, "\n")

# Create a cumulative variance plot
library(ggplot2)
variance_df <- data.frame(
  PC = 1:length(cumulative_variance),
  Variance = cumulative_variance
)

ggplot(variance_df, aes(x = PC, y = Variance)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.7, linetype = "dashed", color = "red") +
  labs(
    title = "Cumulative Variance Explained by Principal Components",
    x = "Principal Components",
    y = "Cumulative Variance Explained"
  ) +
  theme_minimal()

# Create a 3D PCA plot with the top PCs
library(plotly)

pca_top_df <- as.data.frame(pca_result$x[, 1:pcs_needed]) # Include PCs explaining > 70% variance
pca_top_df$cluster <- country_summary$cluster

plot_ly(
  data = pca_top_df,
  x = ~PC1,
  y = ~PC2,
  z = ~PC3,
  color = ~cluster,
  colors = "Spectral",
  type = "scatter3d",
  mode = "markers"
) %>%
  layout(
    title = paste("3D PCA Plot: PCs Explaining", round(cumulative_variance[pcs_needed] * 100, 2), "% Variance"),
    scene = list(
      xaxis = list(title = "Principal Component 1"),
      yaxis = list(title = "Principal Component 2"),
      zaxis = list(title = "Principal Component 3")
    )
  )

```

#### Summary characteristics for clusters

```{r}
# Summarize song attributes by cluster
cluster_summary <- country_summary %>%
  group_by(cluster) %>%
  summarize(
    avg_acousticness = mean(avg_acousticness, na.rm = TRUE),
    avg_danceability = mean(avg_danceability, na.rm = TRUE),
    avg_valence = mean(avg_valence, na.rm = TRUE),
    avg_energy = mean(avg_energy, na.rm = TRUE),
    avg_loudness = mean(avg_loudness, na.rm = TRUE),
    avg_instrumentalness = mean(avg_instrumentalness, na.rm = TRUE),
    avg_tempo = mean(avg_tempo, na.rm = TRUE),
    avg_liveness = mean(avg_liveness, na.rm = TRUE)
  )
print(cluster_summary, width = Inf)
```

#### Visualizing Attribute Differences across clusters

```{r}
# Melt data for visualization
library(reshape2)
cluster_summary_melted <- melt(cluster_summary, id.vars = "cluster")

# Plot attributes by cluster
cluster_colors <- RColorBrewer::brewer.pal(n = 5, name = "Spectral")
ggplot(cluster_summary_melted, aes(x = cluster, y = value, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~variable, scales = "free", ncol = 3) +
  scale_fill_manual(values = cluster_colors)
  labs(
    title = "Comparison of Song Attributes Across Clusters",
    x = "Cluster",
    y = "Average Value"
  ) +
  theme_minimal()
```

```{r}
# Load world map and merge with clustering results
library(rnaturalearth)
library(rnaturalearthdata)
library(ggplot2)
library(RColorBrewer)
library(countrycode)

# Create a mapping table for country codes and names
country_summary <- country_summary %>%
  mutate(
    name = countrycode(country, origin = "iso2c", destination = "country.name")
  )

world <- ne_countries(scale = "medium", returnclass = "sf")

# Merge clustering results with the world map using ISO 2
world_clusters <- world %>%
  mutate(iso_a2 = countrycode(admin, origin = "country.name", destination = "iso2c")) %>%
  left_join(country_summary, by = c("iso_a2" = "country"))

# Plot the world map with clusters
ggplot(data = world_clusters) +
  geom_sf(aes(fill = cluster)) +
  scale_fill_brewer(palette = "Spectral", na.value = "grey80") +
  labs(
    title = "Clusters of Countries by Musical Preferences",
    fill = "Cluster"
  ) +
  theme_minimal() +
  theme(
    panel.background = element_rect(fill = "lightblue"),
    plot.title = element_text(size = 16, face = "bold"),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )
```

#### Top Artists in Each cluster

```{r}
# Top artists by cluster
top_artists <- spotify_df %>%
  left_join(country_summary %>% select(country, cluster), by = "country") %>%
  group_by(cluster, artists) %>%
  summarize(
    avg_popularity = mean(popularity, na.rm = TRUE),
    song_count = n()
  ) %>%
  arrange(cluster, desc(song_count)) %>%
  group_by(cluster) %>%
  slice_head(n = 5)

print(top_artists)

# Visualize top artists by cluster
library(ggplot2)

ggplot(top_artists, aes(x = reorder(artists, -song_count), y = song_count, fill = cluster)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ cluster, scales = "free", ncol = 2) +
  scale_fill_manual(values = cluster_colors) +
  coord_flip() +
  labs(
    title = "Top 5 Artists in Each Cluster",
    x = "Artist",
    y = "Song Count",
    fill = "Cluster"
  ) +
  theme_minimal()

```

#### How does song popularity vary over time within clusters?

```{r}
# Aggregate popularity by date and cluster
popularity_trends <- spotify_df %>%
  left_join(country_summary %>% select(country, cluster), by = "country") %>%
  group_by(snapshot_date, cluster) %>%
  summarize(avg_popularity = mean(popularity, na.rm = TRUE)) %>%
  ungroup()

# Plot popularity trends over time
library(ggplot2)
ggplot(popularity_trends, aes(x = snapshot_date, y = avg_popularity, color = cluster)) +
  geom_line(size = 1) +
  scale_fill_manual(values = cluster_colors) + 
  labs(
    title = "Popularity Trends Over Time by Cluster",
    x = "Date",
    y = "Average Popularity",
    color = "Cluster"
  ) +
  theme_minimal()
```

#### Daily and Weekly Movement Trends

```{r}

# Variability in daily movement by cluster
rank_stability <- spotify_df %>%
  left_join(country_summary %>% select(country, cluster), by = "country") %>%
  group_by(cluster) %>%
  summarize(
    avg_daily_movement = mean(abs(daily_movement), na.rm = TRUE),
    avg_weekly_movement = mean(abs(weekly_movement), na.rm = TRUE)
  )

# Plot rank stability
library(reshape2)
rank_stability_melted <- melt(rank_stability, id.vars = "cluster")

ggplot(rank_stability_melted, aes(x = cluster, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Rank Stability Across Clusters",
    x = "Cluster",
    y = "Average Movement",
    fill = "Metric"
  ) +
  theme_minimal()
```


#### STATISTICAL TESTS

#### 1. One-Way Anova to test if the mean values of each attribute differ significantly across clusters

```{r}

# Attributes
attributes <- c("avg_loudness", "avg_tempo", "avg_valence", "avg_danceability",  "avg_instrumentalness", "avg_acousticness", "avg_liveness")

# Loop through attributes and perform ANOVA
anova_results <- lapply(attributes, function(attr) {
  # Dynamically create the formula for ANOVA
  formula <- as.formula(paste(attr, "~ cluster"))
  # Perform ANOVA
  anova_res <- aov(formula, data = country_summary)
  # Return a summary of ANOVA results
  list(Attribute = attr, ANOVA = summary(anova_res))
})

# Print the results for each attribute
for (res in anova_results) {
  cat("\n\n*** ANOVA Results for", res$Attribute, "***\n")
  print(res$ANOVA)
}
```

#### 2. Chi-Square Test of Independence for explicitness across clusters

```{r}

# Prepare data: Ensure `cluster` and `is_explicit` are non-NA and categorical
explicit_data <- spotify_df %>%
  left_join(country_summary %>% select(country, cluster), by = "country") %>%
  filter(!is.na(cluster) & !is.na(is_explicit)) %>%
  mutate(is_explicit = as.factor(is_explicit))

# Create a contingency table
explicit_cluster_table <- table(explicit_data$cluster, explicit_data$is_explicit)

# Perform the Chi-Square Test
chi_square_result <- chisq.test(explicit_cluster_table)

# Output the results
cat("\n\n*** Chi-Square Test Results ***\n")
cat("Chi-Square Statistic:", chi_square_result$statistic, "\n")
cat("p-value:", chi_square_result$p.value, "\n")
cat("Degrees of Freedom:", chi_square_result$parameter, "\n")
cat("Expected Frequencies:\n")
print(chi_square_result$expected)

# Visualize the proportions of explicit content across clusters
library(ggplot2)
explicit_data_frame <- as.data.frame(explicit_cluster_table)
colnames(explicit_data_frame) <- c("Cluster", "Explicit", "Count")

ggplot(explicit_data_frame, aes(x = Cluster, y = Count, fill = Explicit)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Distribution of Explicit Content Across Clusters",
    x = "Cluster",
    y = "Count",
    fill = "Explicit Content"
  ) +
  theme_minimal()


```
4. What temporal patterns exist in rise and fall of song popularity?
### Temporal Analysis

##Temporal Trends in Song Popularity
#Aggregate Average Popularity Over Time
```{r}
top_songs <- top_songs %>%
  mutate(
    snapshot_date = as.Date('2024-01-01') - sample(1:365, n(), replace = TRUE),
    popularity = sample(50:100, n(), replace = TRUE)
  )

# Aggregate average popularity over time
popularity_trend <- top_songs %>%
  group_by(snapshot_date) %>%
  summarize(avg_popularity = mean(popularity, na.rm = TRUE), .groups = "drop")

# Plot the trend
ggplot(popularity_trend, aes(x = snapshot_date, y = avg_popularity)) +
  geom_line(color = "blue") +
  labs(
    title = "Trend in Average Song Popularity Over Time",
    x = "Snapshot Date",
    y = "Average Popularity"
  ) +
  theme_minimal()
```


### Advanced Analysis

# ANOVA: Release year vs. Popularity
```{r}
# Simulate release years for the songs (e.g., between 2000 and 2023)
set.seed(123)  # Ensure reproducibility
top_songs <- top_songs %>%
  mutate(
    release_year = sample(2000:2023, size = nrow(top_songs), replace = TRUE)
  )
# Perform ANOVA to test if release year impacts popularity
anova_test <- aov(n ~ release_year, data = top_songs)  # Replace 'n' with your popularity metric
summary(anova_test)
ggplot(top_songs, aes(x = as.factor(release_year), y = n)) +
  geom_boxplot(fill = "lightblue") +
  labs(
    title = "Popularity Across Release Years",
    x = "Release Year",
    y = "Popularity"
  ) +
  theme_minimal()

```


# Correlation: Days since release and popularity
```{r}
set.seed(123)
top_songs <- top_songs %>%
  mutate(
    snapshot_date = as.Date('2023-01-01') + sample(1:365, nrow(top_songs), replace = TRUE),
    album_release_date = snapshot_date - sample(1:365, nrow(top_songs), replace = TRUE),
    days_since_release = as.numeric(snapshot_date - album_release_date)
  )
# Perform Pearson correlation test
cor_test <- cor.test(top_songs$days_since_release, top_songs$n, method = "pearson")
print(cor_test)
```


# K-means clustering: Popularity trajectories
```{r}
# Group by song name and snapshot_date
popularity_matrix <- top_songs %>%
  group_by(name, snapshot_date) %>%
  summarize(avg_popularity = mean(n, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = snapshot_date, values_from = avg_popularity, values_fill = 0)

class(popularity_matrix)
colnames(popularity_matrix)

popularity_matrix <- as.data.frame(popularity_matrix)

numeric_matrix <- popularity_matrix[, -1] 

# Perform K-means clustering
set.seed(123)
kmeans_result <- kmeans(scale(numeric_matrix), centers = 3)
popularity_matrix$cluster <- as.factor(kmeans_result$cluster)
popularity_long <- popularity_matrix %>%
  pivot_longer(
    cols = -c(name, cluster),
    names_to = "snapshot_date",
    values_to = "avg_popularity"
  )
popularity_long$snapshot_date <- as.Date(popularity_long$snapshot_date)

popularity_summary <- popularity_long %>%
  group_by(snapshot_date, cluster) %>%
  summarize(avg_popularity = mean(avg_popularity, na.rm = TRUE), .groups = "drop")
ggplot(popularity_summary, aes(x = snapshot_date, y = avg_popularity, color = cluster)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE) +
  labs(
    title = "Average Popularity Trends by Cluster",
    x = "Snapshot Date",
    y = "Average Popularity"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

```



# T-test: Global vs. regional popularity
```{r}
top_songs <- top_songs %>%
  mutate(country = ifelse(n > median(n, na.rm = TRUE), "Global", "Regional"),
         is_global = ifelse(country == "Global", "Global", "Regional"))
t_test <- t.test(n ~ is_global, data = top_songs, alternative = "two.sided")
print(t_test)
mean_summary <- top_songs %>%
  group_by(is_global) %>%
  summarize(mean_n = mean(n, na.rm = TRUE), .groups = "drop")

print(mean_summary)
```
